# Docker Compose for Spark integration testing with Bingsan
# Usage:
#   docker compose -f deployments/docker/docker-compose-spark.yml up -d
#   docker compose -f deployments/docker/docker-compose-spark.yml exec spark-test pytest /tests/test_spark.py -v

services:
  # PostgreSQL for Bingsan metadata
  postgres:
    image: postgres:16-alpine
    environment:
      POSTGRES_USER: bingsan
      POSTGRES_PASSWORD: bingsan
      POSTGRES_DB: bingsan
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U bingsan"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - spark-net

  # MinIO for S3-compatible storage
  minio:
    image: minio/minio:latest
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    healthcheck:
      test: ["CMD", "mc", "ready", "local"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - spark-net

  # Create MinIO buckets
  minio-init:
    image: minio/mc:latest
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: |
      /bin/sh -c "
      mc alias set local http://minio:9000 minioadmin minioadmin;
      mc mb local/warehouse --ignore-existing;
      mc anonymous set public local/warehouse;
      exit 0;
      "
    networks:
      - spark-net

  # Bingsan catalog service
  catalog:
    build:
      context: ../..
      dockerfile: deployments/docker/Dockerfile
    environment:
      BINGSAN_SERVER_HOST: "0.0.0.0"
      BINGSAN_SERVER_PORT: "8181"
      BINGSAN_DATABASE_HOST: postgres
      BINGSAN_DATABASE_PORT: "5432"
      BINGSAN_DATABASE_USER: bingsan
      BINGSAN_DATABASE_PASSWORD: bingsan
      BINGSAN_DATABASE_NAME: bingsan
      BINGSAN_STORAGE_TYPE: s3
      BINGSAN_STORAGE_WAREHOUSE: s3://warehouse/
      BINGSAN_STORAGE_S3_ENDPOINT: http://minio:9000
      BINGSAN_STORAGE_S3_ACCESS_KEY: minioadmin
      BINGSAN_STORAGE_S3_SECRET_KEY: minioadmin
      BINGSAN_STORAGE_S3_PATH_STYLE: "true"
    depends_on:
      postgres:
        condition: service_healthy
      minio-init:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8181/health"]
      interval: 5s
      timeout: 5s
      retries: 10
    networks:
      - spark-net

  # Spark master
  spark-master:
    image: bitnami/spark:3.5
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
    networks:
      - spark-net

  # Spark worker
  spark-worker:
    image: bitnami/spark:3.5
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    depends_on:
      - spark-master
    networks:
      - spark-net

  # Spark test runner with PySpark and Iceberg
  spark-test:
    image: bitnami/spark:3.5
    environment:
      ICEBERG_CATALOG_URI: http://catalog:8181
      ICEBERG_S3_ENDPOINT: http://minio:9000
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin
      AWS_REGION: us-east-1
    volumes:
      - ../../tests/integration:/tests:ro
    depends_on:
      catalog:
        condition: service_healthy
      spark-master:
        condition: service_started
    entrypoint: |
      /bin/bash -c "
      pip install pytest pyspark;
      # Download Iceberg Spark runtime JAR
      wget -q -P /opt/bitnami/spark/jars/ https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.5.2/iceberg-spark-runtime-3.5_2.12-1.5.2.jar;
      wget -q -P /opt/bitnami/spark/jars/ https://repo1.maven.org/maven2/software/amazon/awssdk/bundle/2.20.18/bundle-2.20.18.jar;
      wget -q -P /opt/bitnami/spark/jars/ https://repo1.maven.org/maven2/software/amazon/awssdk/url-connection-client/2.20.18/url-connection-client-2.20.18.jar;
      # Keep container running
      tail -f /dev/null
      "
    networks:
      - spark-net

networks:
  spark-net:
    driver: bridge
